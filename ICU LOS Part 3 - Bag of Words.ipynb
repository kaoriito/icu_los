{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third part of predicting the ICU length of stay, we will perform a bag of words approach on the physician and nursing notes. This method will count up the frequency of word occurences as a predictors of ICU LOS.\n",
    "\n",
    "NOTE: much of the code here is based on [this](https://blog.insightdatascience.com/introduction-to-clinical-natural-language-processing-c563b773053f) tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish BOW, we need to do the following:\n",
    "    1. split data into training/validation sets (because we only want to use train with words in the training set!)\n",
    "    2. parse all of the words, removing \\n or \\r, and fill in missing notes with a space\n",
    "    3. get all of the unique words in the training set\n",
    "    4. count the number of times each word occurs for each row\n",
    "    5. lemmatize / stem words (words ending with -ing, or pluarals, or past-tense should be counted as the same word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in preprocessed data from part 2\n",
    "df_icu = pd.read_csv('data/ICU_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>LOS</th>\n",
       "      <th>INTIME</th>\n",
       "      <th>OUTTIME</th>\n",
       "      <th>ETHNICITY_WHITE</th>\n",
       "      <th>ETHNICITY_BLACK</th>\n",
       "      <th>ETHNICITY_HISPANIC/LATINO</th>\n",
       "      <th>ETHNICITY_ASIAN</th>\n",
       "      <th>...</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_HOSP/EXTRAM</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR</th>\n",
       "      <th>INSURANCE_Government</th>\n",
       "      <th>INSURANCE_Medicaid</th>\n",
       "      <th>INSURANCE_Medicare</th>\n",
       "      <th>INSURANCE_Private</th>\n",
       "      <th>INSURANCE_Self_Pay</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SUBJECT_ID, HADM_ID, ICUSTAY_ID, LOS, INTIME, OUTTIME, ETHNICITY_WHITE, ETHNICITY_BLACK, ETHNICITY_HISPANIC/LATINO, ETHNICITY_ASIAN, ETHNICITY_NATIVE, ADMISSION_NUM, ICUSTAY_NUM, AGE, ICD_BINS_18, ICD_BINS_1, ICD_BINS_2, ICD_BINS_3, ICD_BINS_4, ICD_BINS_5, ICD_BINS_6, ICD_BINS_7, ICD_BINS_8, ICD_BINS_9, ICD_BINS_10, ICD_BINS_11, ICD_BINS_12, ICD_BINS_13, ICD_BINS_14, ICD_BINS_15, ICD_BINS_16, ICD_BINS_17, COMORBID_TOTAL, ABNORMAL_EVENTS_TOTAL, ADMISSION_ELECTIVE, ADMISSION_EMERGENCY, ADMISSION_URGENT, ADMIT_LOCATION_CLINIC_REFERRAL/PREMATURE, ADMIT_LOCATION_EMERGENCY_ROOM_ADMIT, ADMIT_LOCATION_PHYS_REFERRAL/NORMAL_DELI, ADMIT_LOCATION_TRANSFER_FROM_HOSP/EXTRAM, ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT, ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR, INSURANCE_Government, INSURANCE_Medicaid, INSURANCE_Medicare, INSURANCE_Private, INSURANCE_Self_Pay, F, M]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_icu.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icu.OUTTIME = pd.to_datetime(df_icu.OUTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "df_icu.INTIME = pd.to_datetime(df_icu.INTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyito/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# get physician data\n",
    "df_notes = pd.read_csv('data/NOTEEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ROW_ID, SUBJECT_ID, HADM_ID, CHARTDATE, CHARTTIME, STORETIME, CATEGORY, DESCRIPTION, CGID, ISERROR, TEXT]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the types of notes there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Discharge summary', 'Echo', 'ECG', 'Nursing', 'Physician ',\n",
       "       'Rehab Services', 'Case Management ', 'Respiratory ', 'Nutrition',\n",
       "       'General', 'Social Work', 'Pharmacy', 'Consult', 'Radiology',\n",
       "       'Nursing/other'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes['CATEGORY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract physician and nursing notes (apparently some nursing notes ended up in nursing/other so extract that too)\n",
    "\n",
    "Originally meant to only extract physician OR only nursing notes, but it turns out that they only have ~8000 entries each... which would mean missing out on a lot of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_phys = df_notes.loc[(df_notes['CATEGORY']=='Physician ') | (df_notes['CATEGORY']=='Nursing') | (df_notes['CATEGORY']=='Nursing/other')  ][['ROW_ID','SUBJECT_ID','HADM_ID','CHARTTIME','TEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ROW_ID, SUBJECT_ID, HADM_ID, CHARTTIME, TEXT]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_phys.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_phys.CHARTTIME = pd.to_datetime(df_notes_phys.CHARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to match the times of the notes for each patient with the ICU in/out times. First merge the dataframes, then filter out the ones that aren't within the time frame. There is a possibility that the note is written after the ICU stay, but for simplicity, we'll stick with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_icu.merge(df_notes_phys,  how = 'inner', on = ['HADM_ID','SUBJECT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_joined.query('CHARTTIME >= INTIME and CHARTTIME <= OUTTIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_grouped = df_joined.groupby(['SUBJECT_ID','HADM_ID','ICUSTAY_ID'], as_index = False).agg({'TEXT': ''.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SUBJECT_ID, HADM_ID, ICUSTAY_ID, TEXT]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_grouped.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_notes_grouped['ICUSTAY_ID'].nunique() == df_joined['ICUSTAY_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32740"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined = pd.merge(df_icu, df_notes_grouped, on=['SUBJECT_ID','HADM_ID','ICUSTAY_ID'])\n",
    "df_joined['ICUSTAY_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>LOS</th>\n",
       "      <th>INTIME</th>\n",
       "      <th>OUTTIME</th>\n",
       "      <th>ETHNICITY_WHITE</th>\n",
       "      <th>ETHNICITY_BLACK</th>\n",
       "      <th>ETHNICITY_HISPANIC/LATINO</th>\n",
       "      <th>ETHNICITY_ASIAN</th>\n",
       "      <th>...</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR</th>\n",
       "      <th>INSURANCE_Government</th>\n",
       "      <th>INSURANCE_Medicaid</th>\n",
       "      <th>INSURANCE_Medicare</th>\n",
       "      <th>INSURANCE_Private</th>\n",
       "      <th>INSURANCE_Self_Pay</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SUBJECT_ID, HADM_ID, ICUSTAY_ID, LOS, INTIME, OUTTIME, ETHNICITY_WHITE, ETHNICITY_BLACK, ETHNICITY_HISPANIC/LATINO, ETHNICITY_ASIAN, ETHNICITY_NATIVE, ADMISSION_NUM, ICUSTAY_NUM, AGE, ICD_BINS_18, ICD_BINS_1, ICD_BINS_2, ICD_BINS_3, ICD_BINS_4, ICD_BINS_5, ICD_BINS_6, ICD_BINS_7, ICD_BINS_8, ICD_BINS_9, ICD_BINS_10, ICD_BINS_11, ICD_BINS_12, ICD_BINS_13, ICD_BINS_14, ICD_BINS_15, ICD_BINS_16, ICD_BINS_17, COMORBID_TOTAL, ABNORMAL_EVENTS_TOTAL, ADMISSION_ELECTIVE, ADMISSION_EMERGENCY, ADMISSION_URGENT, ADMIT_LOCATION_CLINIC_REFERRAL/PREMATURE, ADMIT_LOCATION_EMERGENCY_ROOM_ADMIT, ADMIT_LOCATION_PHYS_REFERRAL/NORMAL_DELI, ADMIT_LOCATION_TRANSFER_FROM_HOSP/EXTRAM, ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT, ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR, INSURANCE_Government, INSURANCE_Medicaid, INSURANCE_Medicare, INSURANCE_Private, INSURANCE_Self_Pay, F, M, TEXT]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 51 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into train/validation sets\n",
    "__IMPORTANT:__ because there are multiple ICU stays per person, we need to make sure the same person does not get split between the train/test sets. This is important for preventing data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "y = df_joined['LOS']\n",
    "X = df_joined.drop(['LOS'], axis = 1)\n",
    "\n",
    "group = df_joined['SUBJECT_ID']\n",
    "\n",
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=0.33, n_splits=2, random_state = 20210615).split(df_joined, groups=df_joined['SUBJECT_ID']))\n",
    "\n",
    "X_train = X.iloc[train_inds]\n",
    "X_test = X.iloc[test_inds]\n",
    "y_train = y.iloc[train_inds]\n",
    "y_test = y.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_merge = X_train.merge(X_test, how='inner', on=['SUBJECT_ID','HADM_ID','ICUSTAY_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>INTIME_x</th>\n",
       "      <th>OUTTIME_x</th>\n",
       "      <th>ETHNICITY_WHITE_x</th>\n",
       "      <th>ETHNICITY_BLACK_x</th>\n",
       "      <th>ETHNICITY_HISPANIC/LATINO_x</th>\n",
       "      <th>ETHNICITY_ASIAN_x</th>\n",
       "      <th>ETHNICITY_NATIVE_x</th>\n",
       "      <th>...</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT_y</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR_y</th>\n",
       "      <th>INSURANCE_Government_y</th>\n",
       "      <th>INSURANCE_Medicaid_y</th>\n",
       "      <th>INSURANCE_Medicare_y</th>\n",
       "      <th>INSURANCE_Private_y</th>\n",
       "      <th>INSURANCE_Self_Pay_y</th>\n",
       "      <th>F_y</th>\n",
       "      <th>M_y</th>\n",
       "      <th>TEXT_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SUBJECT_ID, HADM_ID, ICUSTAY_ID, INTIME_x, OUTTIME_x, ETHNICITY_WHITE_x, ETHNICITY_BLACK_x, ETHNICITY_HISPANIC/LATINO_x, ETHNICITY_ASIAN_x, ETHNICITY_NATIVE_x, ADMISSION_NUM_x, ICUSTAY_NUM_x, AGE_x, ICD_BINS_18_x, ICD_BINS_1_x, ICD_BINS_2_x, ICD_BINS_3_x, ICD_BINS_4_x, ICD_BINS_5_x, ICD_BINS_6_x, ICD_BINS_7_x, ICD_BINS_8_x, ICD_BINS_9_x, ICD_BINS_10_x, ICD_BINS_11_x, ICD_BINS_12_x, ICD_BINS_13_x, ICD_BINS_14_x, ICD_BINS_15_x, ICD_BINS_16_x, ICD_BINS_17_x, COMORBID_TOTAL_x, ABNORMAL_EVENTS_TOTAL_x, ADMISSION_ELECTIVE_x, ADMISSION_EMERGENCY_x, ADMISSION_URGENT_x, ADMIT_LOCATION_CLINIC_REFERRAL/PREMATURE_x, ADMIT_LOCATION_EMERGENCY_ROOM_ADMIT_x, ADMIT_LOCATION_PHYS_REFERRAL/NORMAL_DELI_x, ADMIT_LOCATION_TRANSFER_FROM_HOSP/EXTRAM_x, ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT_x, ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR_x, INSURANCE_Government_x, INSURANCE_Medicaid_x, INSURANCE_Medicare_x, INSURANCE_Private_x, INSURANCE_Self_Pay_x, F_x, M_x, TEXT_x, INTIME_y, OUTTIME_y, ETHNICITY_WHITE_y, ETHNICITY_BLACK_y, ETHNICITY_HISPANIC/LATINO_y, ETHNICITY_ASIAN_y, ETHNICITY_NATIVE_y, ADMISSION_NUM_y, ICUSTAY_NUM_y, AGE_y, ICD_BINS_18_y, ICD_BINS_1_y, ICD_BINS_2_y, ICD_BINS_3_y, ICD_BINS_4_y, ICD_BINS_5_y, ICD_BINS_6_y, ICD_BINS_7_y, ICD_BINS_8_y, ICD_BINS_9_y, ICD_BINS_10_y, ICD_BINS_11_y, ICD_BINS_12_y, ICD_BINS_13_y, ICD_BINS_14_y, ICD_BINS_15_y, ICD_BINS_16_y, ICD_BINS_17_y, COMORBID_TOTAL_y, ABNORMAL_EVENTS_TOTAL_y, ADMISSION_ELECTIVE_y, ADMISSION_EMERGENCY_y, ADMISSION_URGENT_y, ADMIT_LOCATION_CLINIC_REFERRAL/PREMATURE_y, ADMIT_LOCATION_EMERGENCY_ROOM_ADMIT_y, ADMIT_LOCATION_PHYS_REFERRAL/NORMAL_DELI_y, ADMIT_LOCATION_TRANSFER_FROM_HOSP/EXTRAM_y, ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT_y, ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR_y, INSURANCE_Government_y, INSURANCE_Medicaid_y, INSURANCE_Medicare_y, INSURANCE_Private_y, INSURANCE_Self_Pay_y, F_y, M_y, TEXT_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 97 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that there were no subjects after using an inner merge\n",
    "X_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating our own tokenizer, which will take strings and break them up at each space so each word is a token. We'll also use a porter stemmer to lemmatize like words. We'll also remove punctuation and any numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_tokenizer(text):\n",
    "    ps = PorterStemmer()\n",
    "    # create a dictionary mapping of characters \n",
    "    # we want to replace with what to replace with\n",
    "    punc_list = string.punctuation+'0123456789'\n",
    "    # maketrans() creates a mapping of the character's Unicode ordinal \n",
    "    t = text.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    text = text.lower().translate(t)\n",
    "    tokenized = word_tokenize(text)\n",
    "    \n",
    "    tokenized_stemmed = [ps.stem(word) for word in tokenized]\n",
    "    \n",
    "    return tokenized_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['these', 'string', 'should', 'be', 'token']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "new_tokenizer('These strings shoulD be tokenized 1/15/2021!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a count vectorizer to learn the words in the traning data (fit) and transform the data so it creates counts for each word, constrain # of words included in vectorizer which will use the top N most frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Âµs, sys: 1e+03 ns, total: 5 Âµs\n",
      "Wall time: 16.9 Âµs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyito/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=3000,\n",
       "                tokenizer=<function new_tokenizer at 0x7f8b3b58f9e0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(max_features = 3000, tokenizer = new_tokenizer)\n",
    "\n",
    "vect.fit(X_train.TEXT.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts for each word\n",
    "doc_matrix = vect.transform(X_train.TEXT.values)\n",
    "term_freq = np.sum(doc_matrix, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = np.squeeze(np.asarray(term_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>to</td>\n",
       "      <td>2796661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>2519236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>with</td>\n",
       "      <td>1694318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>1561204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pt</td>\n",
       "      <td>1375696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fire</td>\n",
       "      <td>2388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>drive</td>\n",
       "      <td>2385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>center</td>\n",
       "      <td>2384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pneumon</td>\n",
       "      <td>2384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fena</td>\n",
       "      <td>2382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tf\n",
       "to       2796661\n",
       "and      2519236\n",
       "with     1694318\n",
       "of       1561204\n",
       "pt       1375696\n",
       "...          ...\n",
       "fire        2388\n",
       "drive       2385\n",
       "center      2384\n",
       "pneumon     2384\n",
       "fena        2382\n",
       "\n",
       "[3000 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = np.squeeze(np.asarray(term_freq))\n",
    "tf_df = pd.DataFrame([tf], columns = vect.get_feature_names()).transpose()\n",
    "tf_df.columns = ['tf']\n",
    "tf_df.sort_values(by='tf',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.Series(tf_df.tf, index = tf_df.index).sort_values(ascending = False)\n",
    "ax = d[:50].plot(kind='bar', figsize=(10,8), width=.8, fontsize=14, rot=90, color = 'b')\n",
    "ax.title.set_size(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=['to','and','with','of','pt','for','on','s','in','ml','mg','o',\n",
    "            'at','the','l','as','a','pm','is','dl','he','she','q','h','by','his',\n",
    "            'her','k','be','but','was','name','patient','am','there','that','are',\n",
    "            'an','also', 'c','t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyito/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/lilyito/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['hi', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=3000,\n",
       "                stop_words=['to', 'and', 'with', 'of', 'pt', 'for', 'on', 's',\n",
       "                            'in', 'ml', 'mg', 'o', 'at', 'the', 'l', 'as', 'a',\n",
       "                            'pm', 'is', 'dl', 'he', 'she', 'q', 'h', 'by',\n",
       "                            'his', 'her', 'k', 'be', 'but', ...],\n",
       "                tokenizer=<function new_tokenizer at 0x7f8b3b58f9e0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add in the stop words and refit\n",
    "vect = CountVectorizer(max_features = 3000, \n",
    "                       tokenizer = new_tokenizer, \n",
    "                       stop_words = stop_words)\n",
    "\n",
    "vect.fit(X_train.TEXT.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = vect.transform(X_train.TEXT.values)\n",
    "X_test_text = vect.transform(X_test.TEXT.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aaa',\n",
       " 'aaox',\n",
       " 'ab',\n",
       " 'abcess',\n",
       " 'abd',\n",
       " 'abdomen',\n",
       " 'abdomin',\n",
       " 'abg',\n",
       " 'abil',\n",
       " 'abl',\n",
       " 'ablat',\n",
       " 'abnorm',\n",
       " 'about',\n",
       " 'abov',\n",
       " 'abp',\n",
       " 'abras',\n",
       " 'abscess',\n",
       " 'absent',\n",
       " 'abus',\n",
       " 'abx',\n",
       " 'ac',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accessori',\n",
       " 'accid',\n",
       " 'accord',\n",
       " 'accordingli',\n",
       " 'ace',\n",
       " 'acei',\n",
       " 'acetaminophen',\n",
       " 'ach',\n",
       " 'achiev',\n",
       " 'acid',\n",
       " 'acidosi',\n",
       " 'acquir',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activ',\n",
       " 'acut',\n",
       " 'acyclovir',\n",
       " 'ad',\n",
       " 'add',\n",
       " 'addendum',\n",
       " 'addit',\n",
       " 'address',\n",
       " 'adenopathi',\n",
       " 'adeq',\n",
       " 'adequ',\n",
       " 'adhes',\n",
       " 'adjust',\n",
       " 'adl',\n",
       " 'adm',\n",
       " 'admin',\n",
       " 'administ',\n",
       " 'administr',\n",
       " 'admiss',\n",
       " 'admit',\n",
       " 'adren',\n",
       " 'advair',\n",
       " 'advanc',\n",
       " 'aerat',\n",
       " 'aerosol',\n",
       " 'af',\n",
       " 'afeb',\n",
       " 'afebril',\n",
       " 'affect',\n",
       " 'afib',\n",
       " 'aflutt',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'ag',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'agent',\n",
       " 'aggit',\n",
       " 'aggress',\n",
       " 'agit',\n",
       " 'ago',\n",
       " 'agre',\n",
       " 'aicd',\n",
       " 'aid',\n",
       " 'air',\n",
       " 'airleak',\n",
       " 'airway',\n",
       " 'alarm',\n",
       " 'alb',\n",
       " 'albumin',\n",
       " 'albuterol',\n",
       " 'alcohol',\n",
       " 'alert',\n",
       " 'alin',\n",
       " 'alk',\n",
       " 'alkalosi',\n",
       " 'all',\n",
       " 'allergi',\n",
       " 'allopurinol',\n",
       " 'allow',\n",
       " 'almost',\n",
       " 'alo',\n",
       " 'alon',\n",
       " 'along',\n",
       " 'alreadi',\n",
       " 'alt',\n",
       " 'alter',\n",
       " 'altern',\n",
       " 'although',\n",
       " 'amber',\n",
       " 'ambien',\n",
       " 'ambu',\n",
       " 'ambul',\n",
       " 'ami',\n",
       " 'amio',\n",
       " 'amiodaron',\n",
       " 'amlodipin',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'ampicillin',\n",
       " 'amt',\n",
       " 'amylas',\n",
       " 'analysi',\n",
       " 'anaphylaxi',\n",
       " 'anasarca',\n",
       " 'anemia',\n",
       " 'anesthesia',\n",
       " 'aneurysm',\n",
       " 'angina',\n",
       " 'angio',\n",
       " 'angioedema',\n",
       " 'ani',\n",
       " 'anicter',\n",
       " 'anion',\n",
       " 'ankl',\n",
       " 'anoth',\n",
       " 'anox',\n",
       " 'answer',\n",
       " 'ant',\n",
       " 'anterior',\n",
       " 'anteriorli',\n",
       " 'anti',\n",
       " 'antibiot',\n",
       " 'antibodi',\n",
       " 'antibx',\n",
       " 'anticip',\n",
       " 'anticoagul',\n",
       " 'antihypertens',\n",
       " 'anur',\n",
       " 'anxieti',\n",
       " 'anxiou',\n",
       " 'anyth',\n",
       " 'aorta',\n",
       " 'aortic',\n",
       " 'aox',\n",
       " 'ap',\n",
       " 'apex',\n",
       " 'apic',\n",
       " 'apnea',\n",
       " 'appar',\n",
       " 'appear',\n",
       " 'appetit',\n",
       " 'appl',\n",
       " 'appli',\n",
       " 'appreci',\n",
       " 'appropri',\n",
       " 'approx',\n",
       " 'approxim',\n",
       " 'ard',\n",
       " 'area',\n",
       " 'arf',\n",
       " 'argatroban',\n",
       " 'arm',\n",
       " 'around',\n",
       " 'arous',\n",
       " 'arrest',\n",
       " 'arrhythmia',\n",
       " 'arriv',\n",
       " 'art',\n",
       " 'arteri',\n",
       " 'arthriti',\n",
       " 'artifici',\n",
       " 'asa',\n",
       " 'ascend',\n",
       " 'ascit',\n",
       " 'ask',\n",
       " 'asleep',\n",
       " 'asp',\n",
       " 'aspir',\n",
       " 'aspirin',\n",
       " 'assess',\n",
       " 'assist',\n",
       " 'associ',\n",
       " 'ast',\n",
       " 'asthma',\n",
       " 'asymptomat',\n",
       " 'atc',\n",
       " 'ate',\n",
       " 'atelectasi',\n",
       " 'atenolol',\n",
       " 'ativan',\n",
       " 'atn',\n",
       " 'atorvastatin',\n",
       " 'atrial',\n",
       " 'atrium',\n",
       " 'atropin',\n",
       " 'atrov',\n",
       " 'attach',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attent',\n",
       " 'attribut',\n",
       " 'atyp',\n",
       " 'audibl',\n",
       " 'aureu',\n",
       " 'auscult',\n",
       " 'auto',\n",
       " 'autodiures',\n",
       " 'autoflow',\n",
       " 'av',\n",
       " 'avail',\n",
       " 'avoid',\n",
       " 'avr',\n",
       " 'await',\n",
       " 'awak',\n",
       " 'awaken',\n",
       " 'awar',\n",
       " 'awok',\n",
       " 'ax',\n",
       " 'axi',\n",
       " 'axillari',\n",
       " 'azithromycin',\n",
       " 'aztreonam',\n",
       " 'b',\n",
       " 'back',\n",
       " 'bacteremia',\n",
       " 'bacteri',\n",
       " 'bactrim',\n",
       " 'bag',\n",
       " 'bair',\n",
       " 'bal',\n",
       " 'balanc',\n",
       " 'balloon',\n",
       " 'band',\n",
       " 'barrier',\n",
       " 'basal',\n",
       " 'base',\n",
       " 'baselin',\n",
       " 'basilar',\n",
       " 'bath',\n",
       " 'bb',\n",
       " 'bc',\n",
       " 'bcx',\n",
       " 'bear',\n",
       " 'beat',\n",
       " 'becam',\n",
       " 'becaus',\n",
       " 'becom',\n",
       " 'bed',\n",
       " 'bedpan',\n",
       " 'bedrest',\n",
       " 'bedsid',\n",
       " 'been',\n",
       " 'befor',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'begun',\n",
       " 'behavior',\n",
       " 'believ',\n",
       " 'belli',\n",
       " 'below',\n",
       " 'benadryl',\n",
       " 'benefit',\n",
       " 'benign',\n",
       " 'benzo',\n",
       " 'best',\n",
       " 'beta',\n",
       " 'better',\n",
       " 'between',\n",
       " 'bg',\n",
       " 'bibasilar',\n",
       " 'bicarb',\n",
       " 'bid',\n",
       " 'bil',\n",
       " 'bilat',\n",
       " 'bilater',\n",
       " 'bile',\n",
       " 'bili',\n",
       " 'biliari',\n",
       " 'biliou',\n",
       " 'bill',\n",
       " 'bioprosthet',\n",
       " 'biopsi',\n",
       " 'bipap',\n",
       " 'bipolar',\n",
       " 'bisacodyl',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'bka',\n",
       " 'bl',\n",
       " 'black',\n",
       " 'bladder',\n",
       " 'blanket',\n",
       " 'bld',\n",
       " 'ble',\n",
       " 'bleed',\n",
       " 'blister',\n",
       " 'block',\n",
       " 'blocker',\n",
       " 'blood',\n",
       " 'bloodi',\n",
       " 'blue',\n",
       " 'blurri',\n",
       " 'bm',\n",
       " 'bmt',\n",
       " 'bnp',\n",
       " 'board',\n",
       " 'bodi',\n",
       " 'bolu',\n",
       " 'bolus',\n",
       " 'bone',\n",
       " 'boot',\n",
       " 'borderlin',\n",
       " 'both',\n",
       " 'bottl',\n",
       " 'bowel',\n",
       " 'bp',\n",
       " 'bph',\n",
       " 'bpm',\n",
       " 'brace',\n",
       " 'brachial',\n",
       " 'bradi',\n",
       " 'bradycard',\n",
       " 'bradycardia',\n",
       " 'brain',\n",
       " 'branch',\n",
       " 'brbpr',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'breakthrough',\n",
       " 'breast',\n",
       " 'breath',\n",
       " 'bridg',\n",
       " 'brief',\n",
       " 'briefli',\n",
       " 'bright',\n",
       " 'bring',\n",
       " 'brisk',\n",
       " 'briskli',\n",
       " 'broad',\n",
       " 'bromid',\n",
       " 'bronch',\n",
       " 'bronchial',\n",
       " 'bronchiti',\n",
       " 'bronchoscopi',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'bruis',\n",
       " 'bruit',\n",
       " 'bs',\n",
       " 'bue',\n",
       " 'bump',\n",
       " 'bun',\n",
       " 'bundl',\n",
       " 'burn',\n",
       " 'burst',\n",
       " 'buttock',\n",
       " 'bx',\n",
       " 'bypass',\n",
       " 'ca',\n",
       " 'cabg',\n",
       " 'cabgx',\n",
       " 'cad',\n",
       " 'calcium',\n",
       " 'calf',\n",
       " 'call',\n",
       " 'calm',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cancer',\n",
       " 'candid',\n",
       " 'cannula',\n",
       " 'cap',\n",
       " 'capac',\n",
       " 'capsul',\n",
       " 'captopril',\n",
       " 'captur',\n",
       " 'car',\n",
       " 'carcinoma',\n",
       " 'card',\n",
       " 'cardiac',\n",
       " 'cardio',\n",
       " 'cardiogen',\n",
       " 'cardiolog',\n",
       " 'cardiomyopathi',\n",
       " 'cardiovascular',\n",
       " 'cardiovers',\n",
       " 'care',\n",
       " 'careview',\n",
       " 'carevu',\n",
       " 'carotid',\n",
       " 'carvedilol',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cast',\n",
       " 'cataract',\n",
       " 'cath',\n",
       " 'cathet',\n",
       " 'catheter',\n",
       " 'caus',\n",
       " 'caviti',\n",
       " 'cbc',\n",
       " 'cbd',\n",
       " 'cc',\n",
       " 'cco',\n",
       " 'ccu',\n",
       " 'cd',\n",
       " 'cdb',\n",
       " 'cdi',\n",
       " 'cdiff',\n",
       " 'ce',\n",
       " 'ced',\n",
       " 'cefazolin',\n",
       " 'cefepim',\n",
       " 'cefipim',\n",
       " 'ceftaz',\n",
       " 'ceftazidim',\n",
       " 'ceftriaxon',\n",
       " 'celexa',\n",
       " 'cell',\n",
       " 'cellul',\n",
       " 'center',\n",
       " 'central',\n",
       " 'cerebellar',\n",
       " 'cerebr',\n",
       " 'cervic',\n",
       " 'chair',\n",
       " 'chang',\n",
       " 'chart',\n",
       " 'chb',\n",
       " 'check',\n",
       " 'chemo',\n",
       " 'chemotherapi',\n",
       " 'chest',\n",
       " 'chf',\n",
       " 'chief',\n",
       " 'children',\n",
       " 'chill',\n",
       " 'chip',\n",
       " 'chlorhexidin',\n",
       " 'chlorid',\n",
       " 'cholang',\n",
       " 'cholecyst',\n",
       " 'cholecystectomi',\n",
       " 'chronic',\n",
       " 'ci',\n",
       " 'cipro',\n",
       " 'ciprofloxacin',\n",
       " 'cirrhosi',\n",
       " 'citrat',\n",
       " 'ciwa',\n",
       " 'ck',\n",
       " 'ckd',\n",
       " 'ckmb',\n",
       " 'cl',\n",
       " 'clamp',\n",
       " 'clarifi',\n",
       " 'clean',\n",
       " 'cleans',\n",
       " 'clear',\n",
       " 'clearanc',\n",
       " 'clearli',\n",
       " 'climb',\n",
       " 'clindamycin',\n",
       " 'clinic',\n",
       " 'clip',\n",
       " 'clonidin',\n",
       " 'close',\n",
       " 'closer',\n",
       " 'closur',\n",
       " 'clot',\n",
       " 'cloth',\n",
       " 'cloudi',\n",
       " 'clr',\n",
       " 'club',\n",
       " 'cluster',\n",
       " 'cm',\n",
       " 'cmh',\n",
       " 'cmv',\n",
       " 'cn',\n",
       " 'co',\n",
       " 'coag',\n",
       " 'coagulopathi',\n",
       " 'coars',\n",
       " 'cocain',\n",
       " 'cocci',\n",
       " 'coccyx',\n",
       " 'code',\n",
       " 'codein',\n",
       " 'coffe',\n",
       " 'cognit',\n",
       " 'coil',\n",
       " 'colac',\n",
       " 'cold',\n",
       " 'colectomi',\n",
       " 'coli',\n",
       " 'coliti',\n",
       " 'collaps',\n",
       " 'collar',\n",
       " 'collect',\n",
       " 'colon',\n",
       " 'colonoscopi',\n",
       " 'color',\n",
       " 'colostomi',\n",
       " 'comb',\n",
       " 'combin',\n",
       " 'combiv',\n",
       " 'come',\n",
       " 'comfort',\n",
       " 'comm',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'commod',\n",
       " 'common',\n",
       " 'commun',\n",
       " 'compani',\n",
       " 'compar',\n",
       " 'compart',\n",
       " 'compazin',\n",
       " 'compens',\n",
       " 'complain',\n",
       " 'complaint',\n",
       " 'complet',\n",
       " 'complex',\n",
       " 'complianc',\n",
       " 'complic',\n",
       " 'compon',\n",
       " 'compress',\n",
       " 'compromis',\n",
       " 'con',\n",
       " 'concentr',\n",
       " 'concern',\n",
       " 'condit',\n",
       " 'condom',\n",
       " 'confirm',\n",
       " 'confus',\n",
       " 'congest',\n",
       " 'conjunctiv',\n",
       " 'conjunctiva',\n",
       " 'consent',\n",
       " 'consid',\n",
       " 'consider',\n",
       " 'consist',\n",
       " 'consolid',\n",
       " 'constant',\n",
       " 'constip',\n",
       " 'constitut',\n",
       " 'consult',\n",
       " 'cont',\n",
       " 'contact',\n",
       " 'contain',\n",
       " 'contamin',\n",
       " 'contin',\n",
       " 'continu',\n",
       " 'contract',\n",
       " 'contrast',\n",
       " 'contribut',\n",
       " 'control',\n",
       " 'contus',\n",
       " 'convers',\n",
       " 'convert',\n",
       " 'cool',\n",
       " 'cooper',\n",
       " 'copd',\n",
       " 'cope',\n",
       " 'copiou',\n",
       " 'cord',\n",
       " 'cordi',\n",
       " 'corneal',\n",
       " 'coronari',\n",
       " 'correct',\n",
       " 'correl',\n",
       " 'cortisol',\n",
       " 'cough',\n",
       " 'could',\n",
       " 'coumadin',\n",
       " 'count',\n",
       " 'countri',\n",
       " 'coupl',\n",
       " 'cours',\n",
       " 'cover',\n",
       " 'coverag',\n",
       " 'cp',\n",
       " 'cpap',\n",
       " 'cpk',\n",
       " 'cpt',\n",
       " 'cr',\n",
       " 'crackl',\n",
       " 'cramp',\n",
       " 'crani',\n",
       " 'craniotomi',\n",
       " 'cre',\n",
       " 'cream',\n",
       " 'creat',\n",
       " 'creatinin',\n",
       " 'credit',\n",
       " 'crepitu',\n",
       " 'crf',\n",
       " 'cri',\n",
       " 'crisi',\n",
       " 'crit',\n",
       " 'critic',\n",
       " 'crohn',\n",
       " 'cross',\n",
       " 'crrt',\n",
       " 'crush',\n",
       " 'csf',\n",
       " 'csm',\n",
       " 'csru',\n",
       " 'ct',\n",
       " 'cta',\n",
       " 'ctab',\n",
       " 'ctx',\n",
       " 'cuff',\n",
       " 'cultur',\n",
       " 'current',\n",
       " 'curv',\n",
       " 'cv',\n",
       " 'cva',\n",
       " 'cvicu',\n",
       " 'cvl',\n",
       " 'cvp',\n",
       " 'cvvh',\n",
       " 'cvvhd',\n",
       " 'cx',\n",
       " 'cxr',\n",
       " 'cyanosi',\n",
       " 'cycl',\n",
       " 'cyst',\n",
       " 'd',\n",
       " 'daili',\n",
       " 'dampen',\n",
       " 'daptomycin',\n",
       " 'dark',\n",
       " 'data',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'db',\n",
       " 'dc',\n",
       " 'ddx',\n",
       " 'de',\n",
       " 'debrid',\n",
       " 'decadron',\n",
       " 'decid',\n",
       " 'decis',\n",
       " 'declin',\n",
       " 'decompens',\n",
       " 'decompress',\n",
       " 'decreas',\n",
       " 'decub',\n",
       " 'decubitu',\n",
       " 'deep',\n",
       " 'defect',\n",
       " 'defer',\n",
       " 'defici',\n",
       " 'deficit',\n",
       " 'definit',\n",
       " 'degre',\n",
       " 'dehydr',\n",
       " 'delay',\n",
       " 'delir',\n",
       " 'deliri',\n",
       " 'delirium',\n",
       " 'deliveri',\n",
       " 'demand',\n",
       " 'dementia',\n",
       " 'demerol',\n",
       " 'demograph',\n",
       " 'demonstr',\n",
       " 'deni',\n",
       " 'dentit',\n",
       " 'depend',\n",
       " 'deplet',\n",
       " 'depress',\n",
       " 'derm',\n",
       " 'dermat',\n",
       " 'desat',\n",
       " 'desatur',\n",
       " 'describ',\n",
       " 'descript',\n",
       " 'despit',\n",
       " 'detail',\n",
       " 'deterior',\n",
       " 'determin',\n",
       " 'develop',\n",
       " 'devic',\n",
       " 'dexamethason',\n",
       " 'dextros',\n",
       " 'diabet',\n",
       " 'diag',\n",
       " 'diagnos',\n",
       " 'diagnosi',\n",
       " 'diagnost',\n",
       " 'dialysi',\n",
       " 'diamox',\n",
       " 'diaphoresi',\n",
       " 'diaphoret',\n",
       " 'diarrhea',\n",
       " 'diastol',\n",
       " 'diazepam',\n",
       " 'dic',\n",
       " 'did',\n",
       " 'die',\n",
       " 'diet',\n",
       " 'diff',\n",
       " 'differ',\n",
       " 'differenti',\n",
       " 'difficil',\n",
       " 'difficult',\n",
       " 'difficulti',\n",
       " 'diffus',\n",
       " 'dig',\n",
       " 'digit',\n",
       " 'digoxin',\n",
       " 'dilantin',\n",
       " 'dilat',\n",
       " 'dilaudid',\n",
       " 'dilt',\n",
       " 'diltiazem',\n",
       " 'dilut',\n",
       " 'dim',\n",
       " 'dimer',\n",
       " 'diminish',\n",
       " 'dinner',\n",
       " 'dip',\n",
       " 'direct',\n",
       " 'discharg',\n",
       " 'discomfort',\n",
       " 'discontinu',\n",
       " 'discuss',\n",
       " 'diseas',\n",
       " 'disord',\n",
       " 'disori',\n",
       " 'dispo',\n",
       " 'disposit',\n",
       " 'dissect',\n",
       " 'distal',\n",
       " 'distant',\n",
       " 'distend',\n",
       " 'distens',\n",
       " 'distent',\n",
       " 'distress',\n",
       " 'diures',\n",
       " 'diuresi',\n",
       " 'diuret',\n",
       " 'diverticul',\n",
       " 'diverticulosi',\n",
       " 'dizzi',\n",
       " 'dka',\n",
       " 'dm',\n",
       " 'dmii',\n",
       " 'dng',\n",
       " 'dni',\n",
       " 'dnr',\n",
       " 'do',\n",
       " 'dobhoff',\n",
       " 'dobutamin',\n",
       " 'doctor',\n",
       " 'document',\n",
       " 'docus',\n",
       " 'doe',\n",
       " 'doesn',\n",
       " 'done',\n",
       " 'dopa',\n",
       " 'dopamin',\n",
       " 'doppler',\n",
       " 'dorsali',\n",
       " 'dose',\n",
       " 'doubl',\n",
       " 'down',\n",
       " 'downward',\n",
       " 'doze',\n",
       " 'dp',\n",
       " 'dr',\n",
       " 'drain',\n",
       " 'drainag',\n",
       " 'draw',\n",
       " 'drawn',\n",
       " 'dress',\n",
       " 'drg',\n",
       " 'dri',\n",
       " 'drift',\n",
       " 'drink',\n",
       " 'drip',\n",
       " 'drive',\n",
       " 'drng',\n",
       " 'droop',\n",
       " 'drop',\n",
       " 'droplet',\n",
       " 'drsg',\n",
       " 'drug',\n",
       " 'dsd',\n",
       " 'dsg',\n",
       " 'dt',\n",
       " 'dtr',\n",
       " 'duct',\n",
       " 'due',\n",
       " 'dulcolax',\n",
       " 'dull',\n",
       " 'duoden',\n",
       " 'duoderm',\n",
       " 'durat',\n",
       " 'dure',\n",
       " 'dvt',\n",
       " 'dx',\n",
       " 'dye',\n",
       " 'dysfunct',\n",
       " 'dyslipidemia',\n",
       " 'dysphagia',\n",
       " 'dyspnea',\n",
       " 'dysuria',\n",
       " 'e',\n",
       " 'each',\n",
       " 'ear',\n",
       " 'earli',\n",
       " 'earlier',\n",
       " 'easili',\n",
       " 'eat',\n",
       " 'ebl',\n",
       " 'ec',\n",
       " 'ecchymosi',\n",
       " 'ecchymot',\n",
       " 'ecg',\n",
       " 'echo',\n",
       " 'ectopi',\n",
       " 'ed',\n",
       " 'edema',\n",
       " 'edemat',\n",
       " 'edg',\n",
       " 'educ',\n",
       " 'eeg',\n",
       " 'ef',\n",
       " 'effect',\n",
       " 'effort',\n",
       " 'effus',\n",
       " 'egd',\n",
       " 'either',\n",
       " 'eject',\n",
       " 'ekg',\n",
       " 'elbow',\n",
       " 'elect',\n",
       " 'electrolyt',\n",
       " 'elev',\n",
       " 'em',\n",
       " 'embol',\n",
       " 'emboli',\n",
       " 'emerg',\n",
       " 'emesi',\n",
       " 'emot',\n",
       " 'emphas',\n",
       " 'emphysema',\n",
       " 'empir',\n",
       " 'enc',\n",
       " 'encephalopathi',\n",
       " 'encourag',\n",
       " 'end',\n",
       " 'endo',\n",
       " 'endocard',\n",
       " 'endocrin',\n",
       " 'endoscopi',\n",
       " 'endotrach',\n",
       " 'enema',\n",
       " 'english',\n",
       " 'enlarg',\n",
       " 'enough',\n",
       " 'ensur',\n",
       " 'ent',\n",
       " 'enter',\n",
       " 'enterococcu',\n",
       " 'entir',\n",
       " 'environ',\n",
       " 'enzym',\n",
       " 'eo',\n",
       " 'eomi',\n",
       " 'ep',\n",
       " 'epi',\n",
       " 'epicardi',\n",
       " 'epidur',\n",
       " 'epigastr',\n",
       " 'epilepticu',\n",
       " 'episod',\n",
       " 'epistaxi',\n",
       " 'equal',\n",
       " 'er',\n",
       " 'ercp',\n",
       " 'erythema',\n",
       " 'erythemat',\n",
       " 'erythromycin',\n",
       " 'esld',\n",
       " 'esmolol',\n",
       " 'esophag',\n",
       " 'esophagu',\n",
       " 'esp',\n",
       " 'especi',\n",
       " 'esrd',\n",
       " 'essenti',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'etiolog',\n",
       " 'etoh',\n",
       " 'ett',\n",
       " 'euvolem',\n",
       " 'evacu',\n",
       " 'eval',\n",
       " 'evalu',\n",
       " 'evd',\n",
       " 'eve',\n",
       " 'even',\n",
       " 'event',\n",
       " 'eventu',\n",
       " 'everi',\n",
       " 'evid',\n",
       " 'ew',\n",
       " 'ex',\n",
       " 'exacerb',\n",
       " 'exam',\n",
       " 'examin',\n",
       " 'excel',\n",
       " 'except',\n",
       " 'exchang',\n",
       " 'exclud',\n",
       " 'excori',\n",
       " 'exercis',\n",
       " 'exert',\n",
       " 'exp',\n",
       " 'expans',\n",
       " 'expect',\n",
       " 'expector',\n",
       " 'experienc',\n",
       " 'expiratori',\n",
       " 'explain',\n",
       " 'explor',\n",
       " 'express',\n",
       " 'ext',\n",
       " 'extend',\n",
       " 'extens',\n",
       " 'extern',\n",
       " 'extra',\n",
       " 'extract',\n",
       " 'extrem',\n",
       " 'extremeti',\n",
       " 'extub',\n",
       " 'eye',\n",
       " 'f',\n",
       " 'face',\n",
       " 'facial',\n",
       " 'facil',\n",
       " 'factor',\n",
       " 'fail',\n",
       " 'failur',\n",
       " 'faint',\n",
       " 'fair',\n",
       " 'fairli',\n",
       " 'fall',\n",
       " 'famili',\n",
       " 'famotidin',\n",
       " 'far',\n",
       " 'fasciotomi',\n",
       " 'fast',\n",
       " 'father',\n",
       " 'fatigu',\n",
       " 'fax',\n",
       " 'fb',\n",
       " 'febril',\n",
       " 'fecal',\n",
       " 'feed',\n",
       " 'feel',\n",
       " 'feet',\n",
       " 'fell',\n",
       " 'fellow',\n",
       " 'felt',\n",
       " 'fem',\n",
       " 'femal',\n",
       " 'femor',\n",
       " 'femur',\n",
       " 'fen',\n",
       " 'fena',\n",
       " 'fent',\n",
       " 'fentanyl',\n",
       " 'fever',\n",
       " 'few',\n",
       " 'ffp',\n",
       " 'fh',\n",
       " 'fi',\n",
       " 'fib',\n",
       " 'fiber',\n",
       " 'fibril',\n",
       " 'fibrinogen',\n",
       " 'fick',\n",
       " 'field',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print(len(vect.get_feature_names()))\n",
    "\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<22068x3000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11358911 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop identifier columns and then combine ICU data with BOW data\n",
    "X_train = X_train.drop(['TEXT','SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID','INTIME', 'OUTTIME'], axis=1)\n",
    "X_test = X_test.drop(['TEXT','SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID','INTIME', 'OUTTIME'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_sparse = hstack([X_train, X_train_text])\n",
    "X_test_sparse = hstack([X_test, X_test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train_list + vect.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "\n",
    "import scipy\n",
    "\n",
    "scipy.sparse.save_npz('data/X_train_sparse.npz', X_train_sparse)\n",
    "scipy.sparse.save_npz('data/X_test_sparse.npz', X_test_sparse)\n",
    "\n",
    "import pickle\n",
    "with open(\"data/feature_names.txt\", \"wb\") as fp:\n",
    "    pickle.dump(feature_names, fp)\n",
    "\n",
    "with open(\"data/y_train.txt\", \"wb\") as fp:\n",
    "    pickle.dump(y_train, fp)\n",
    "    \n",
    "with open(\"data/y_test.txt\", \"wb\") as fp:\n",
    "    pickle.dump(y_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
