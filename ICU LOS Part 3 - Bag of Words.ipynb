{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third part of predicting the ICU length of stay, we will perform a bag of words approach on the physician and nursing notes. This method will count up the frequency of word occurences as a predictors of ICU LOS.\n",
    "\n",
    "NOTE: much of the code here is based on [this](https://blog.insightdatascience.com/introduction-to-clinical-natural-language-processing-c563b773053f) tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish BOW, we need to do the following:\n",
    "    1. split data into training/validation sets (because we only want to use train with words in the training set!)\n",
    "    2. parse all of the words, removing \\n or \\r, and fill in missing notes with a space\n",
    "    3. get all of the unique words in the training set\n",
    "    4. count the number of times each word occurs for each row\n",
    "    5. lemmatize / stem words (words ending with -ing, or pluarals, or past-tense should be counted as the same word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in preprocessed data from part 2\n",
    "df_icu = pd.read_csv('data/ICU_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>LOS</th>\n",
       "      <th>INTIME</th>\n",
       "      <th>OUTTIME</th>\n",
       "      <th>ETHNICITY_WHITE</th>\n",
       "      <th>ETHNICITY_BLACK</th>\n",
       "      <th>ETHNICITY_HISPANIC/LATINO</th>\n",
       "      <th>ETHNICITY_ASIAN</th>\n",
       "      <th>...</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_HOSP/EXTRAM</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR</th>\n",
       "      <th>INSURANCE_Government</th>\n",
       "      <th>INSURANCE_Medicaid</th>\n",
       "      <th>INSURANCE_Medicare</th>\n",
       "      <th>INSURANCE_Private</th>\n",
       "      <th>INSURANCE_Self_Pay</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SUBJECT_ID, HADM_ID, ICUSTAY_ID, LOS, INTIME, OUTTIME, ETHNICITY_WHITE, ETHNICITY_BLACK, ETHNICITY_HISPANIC/LATINO, ETHNICITY_ASIAN, ETHNICITY_NATIVE, ADMISSION_NUM, ICUSTAY_NUM, AGE, ICD_BINS_18, ICD_BINS_1, ICD_BINS_2, ICD_BINS_3, ICD_BINS_4, ICD_BINS_5, ICD_BINS_6, ICD_BINS_7, ICD_BINS_8, ICD_BINS_9, ICD_BINS_10, ICD_BINS_11, ICD_BINS_12, ICD_BINS_13, ICD_BINS_14, ICD_BINS_15, ICD_BINS_16, ICD_BINS_17, COMORBID_TOTAL, ABNORMAL_EVENTS_TOTAL, ADMISSION_ELECTIVE, ADMISSION_EMERGENCY, ADMISSION_URGENT, ADMIT_LOCATION_CLINIC_REFERRAL/PREMATURE, ADMIT_LOCATION_EMERGENCY_ROOM_ADMIT, ADMIT_LOCATION_PHYS_REFERRAL/NORMAL_DELI, ADMIT_LOCATION_TRANSFER_FROM_HOSP/EXTRAM, ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT, ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR, INSURANCE_Government, INSURANCE_Medicaid, INSURANCE_Medicare, INSURANCE_Private, INSURANCE_Self_Pay, F, M]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_icu.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icu.OUTTIME = pd.to_datetime(df_icu.OUTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "df_icu.INTIME = pd.to_datetime(df_icu.INTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyito/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# get physician data\n",
    "df_notes = pd.read_csv('data/NOTEEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ROW_ID, SUBJECT_ID, HADM_ID, CHARTDATE, CHARTTIME, STORETIME, CATEGORY, DESCRIPTION, CGID, ISERROR, TEXT]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the types of notes there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Discharge summary', 'Echo', 'ECG', 'Nursing', 'Physician ',\n",
       "       'Rehab Services', 'Case Management ', 'Respiratory ', 'Nutrition',\n",
       "       'General', 'Social Work', 'Pharmacy', 'Consult', 'Radiology',\n",
       "       'Nursing/other'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes['CATEGORY'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract physician and nursing notes (apparently some nursing notes ended up in nursing/other so extract that too)\n",
    "\n",
    "Originally meant to only extract physician OR only nursing notes, but it turns out that they only have ~8000 entries each... which would mean missing out on a lot of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_phys = df_notes.loc[(df_notes['CATEGORY']=='Physician ') | (df_notes['CATEGORY']=='Nursing') | (df_notes['CATEGORY']=='Nursing/other')  ][['ROW_ID','SUBJECT_ID','HADM_ID','CHARTTIME','TEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ROW_ID, SUBJECT_ID, HADM_ID, CHARTTIME, TEXT]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_phys.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_phys.CHARTTIME = pd.to_datetime(df_notes_phys.CHARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to match the times of the notes for each patient with the ICU in/out times. First merge the dataframes, then filter out the ones that aren't within the time frame. There is a possibility that the note is written after the ICU stay, but for simplicity, we'll stick with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_icu.merge(df_notes_phys,  how = 'inner', on = ['HADM_ID','SUBJECT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_joined.query('CHARTTIME >= INTIME and CHARTTIME <= OUTTIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_grouped = df_joined.groupby(['SUBJECT_ID','HADM_ID','ICUSTAY_ID'], as_index = False).agg({'TEXT': ''.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SUBJECT_ID, HADM_ID, ICUSTAY_ID, TEXT]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_grouped.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_notes_grouped['ICUSTAY_ID'].nunique() == df_joined['ICUSTAY_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32740"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined = pd.merge(df_icu, df_notes_grouped, on=['SUBJECT_ID','HADM_ID','ICUSTAY_ID'])\n",
    "df_joined['ICUSTAY_ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>LOS</th>\n",
       "      <th>INTIME</th>\n",
       "      <th>OUTTIME</th>\n",
       "      <th>ETHNICITY_WHITE</th>\n",
       "      <th>ETHNICITY_BLACK</th>\n",
       "      <th>ETHNICITY_HISPANIC/LATINO</th>\n",
       "      <th>ETHNICITY_ASIAN</th>\n",
       "      <th>...</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR</th>\n",
       "      <th>INSURANCE_Government</th>\n",
       "      <th>INSURANCE_Medicaid</th>\n",
       "      <th>INSURANCE_Medicare</th>\n",
       "      <th>INSURANCE_Private</th>\n",
       "      <th>INSURANCE_Self_Pay</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SUBJECT_ID, HADM_ID, ICUSTAY_ID, LOS, INTIME, OUTTIME, ETHNICITY_WHITE, ETHNICITY_BLACK, ETHNICITY_HISPANIC/LATINO, ETHNICITY_ASIAN, ETHNICITY_NATIVE, ADMISSION_NUM, ICUSTAY_NUM, AGE, ICD_BINS_18, ICD_BINS_1, ICD_BINS_2, ICD_BINS_3, ICD_BINS_4, ICD_BINS_5, ICD_BINS_6, ICD_BINS_7, ICD_BINS_8, ICD_BINS_9, ICD_BINS_10, ICD_BINS_11, ICD_BINS_12, ICD_BINS_13, ICD_BINS_14, ICD_BINS_15, ICD_BINS_16, ICD_BINS_17, COMORBID_TOTAL, ABNORMAL_EVENTS_TOTAL, ADMISSION_ELECTIVE, ADMISSION_EMERGENCY, ADMISSION_URGENT, ADMIT_LOCATION_CLINIC_REFERRAL/PREMATURE, ADMIT_LOCATION_EMERGENCY_ROOM_ADMIT, ADMIT_LOCATION_PHYS_REFERRAL/NORMAL_DELI, ADMIT_LOCATION_TRANSFER_FROM_HOSP/EXTRAM, ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT, ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR, INSURANCE_Government, INSURANCE_Medicaid, INSURANCE_Medicare, INSURANCE_Private, INSURANCE_Self_Pay, F, M, TEXT]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 51 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into train/validation sets\n",
    "__IMPORTANT:__ because there are multiple ICU stays per person, we need to make sure the same person does not get split between the train/test sets. This is important for preventing data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "y = df_joined['LOS']\n",
    "X = df_joined.drop(['LOS'], axis = 1)\n",
    "\n",
    "group = df_joined['SUBJECT_ID']\n",
    "\n",
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=0.33, n_splits=2, random_state = 20210615).split(df_joined, groups=df_joined['SUBJECT_ID']))\n",
    "\n",
    "X_train = X.iloc[train_inds]\n",
    "X_test = X.iloc[test_inds]\n",
    "y_train = y.iloc[train_inds]\n",
    "y_test = y.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_merge = X_train.merge(X_test, how='inner', on=['SUBJECT_ID','HADM_ID','ICUSTAY_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>INTIME_x</th>\n",
       "      <th>OUTTIME_x</th>\n",
       "      <th>ETHNICITY_WHITE_x</th>\n",
       "      <th>ETHNICITY_BLACK_x</th>\n",
       "      <th>ETHNICITY_HISPANIC/LATINO_x</th>\n",
       "      <th>ETHNICITY_ASIAN_x</th>\n",
       "      <th>ETHNICITY_NATIVE_x</th>\n",
       "      <th>...</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT_y</th>\n",
       "      <th>ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR_y</th>\n",
       "      <th>INSURANCE_Government_y</th>\n",
       "      <th>INSURANCE_Medicaid_y</th>\n",
       "      <th>INSURANCE_Medicare_y</th>\n",
       "      <th>INSURANCE_Private_y</th>\n",
       "      <th>INSURANCE_Self_Pay_y</th>\n",
       "      <th>F_y</th>\n",
       "      <th>M_y</th>\n",
       "      <th>TEXT_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SUBJECT_ID, HADM_ID, ICUSTAY_ID, INTIME_x, OUTTIME_x, ETHNICITY_WHITE_x, ETHNICITY_BLACK_x, ETHNICITY_HISPANIC/LATINO_x, ETHNICITY_ASIAN_x, ETHNICITY_NATIVE_x, ADMISSION_NUM_x, ICUSTAY_NUM_x, AGE_x, ICD_BINS_18_x, ICD_BINS_1_x, ICD_BINS_2_x, ICD_BINS_3_x, ICD_BINS_4_x, ICD_BINS_5_x, ICD_BINS_6_x, ICD_BINS_7_x, ICD_BINS_8_x, ICD_BINS_9_x, ICD_BINS_10_x, ICD_BINS_11_x, ICD_BINS_12_x, ICD_BINS_13_x, ICD_BINS_14_x, ICD_BINS_15_x, ICD_BINS_16_x, ICD_BINS_17_x, COMORBID_TOTAL_x, ABNORMAL_EVENTS_TOTAL_x, ADMISSION_ELECTIVE_x, ADMISSION_EMERGENCY_x, ADMISSION_URGENT_x, ADMIT_LOCATION_CLINIC_REFERRAL/PREMATURE_x, ADMIT_LOCATION_EMERGENCY_ROOM_ADMIT_x, ADMIT_LOCATION_PHYS_REFERRAL/NORMAL_DELI_x, ADMIT_LOCATION_TRANSFER_FROM_HOSP/EXTRAM_x, ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT_x, ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR_x, INSURANCE_Government_x, INSURANCE_Medicaid_x, INSURANCE_Medicare_x, INSURANCE_Private_x, INSURANCE_Self_Pay_x, F_x, M_x, TEXT_x, INTIME_y, OUTTIME_y, ETHNICITY_WHITE_y, ETHNICITY_BLACK_y, ETHNICITY_HISPANIC/LATINO_y, ETHNICITY_ASIAN_y, ETHNICITY_NATIVE_y, ADMISSION_NUM_y, ICUSTAY_NUM_y, AGE_y, ICD_BINS_18_y, ICD_BINS_1_y, ICD_BINS_2_y, ICD_BINS_3_y, ICD_BINS_4_y, ICD_BINS_5_y, ICD_BINS_6_y, ICD_BINS_7_y, ICD_BINS_8_y, ICD_BINS_9_y, ICD_BINS_10_y, ICD_BINS_11_y, ICD_BINS_12_y, ICD_BINS_13_y, ICD_BINS_14_y, ICD_BINS_15_y, ICD_BINS_16_y, ICD_BINS_17_y, COMORBID_TOTAL_y, ABNORMAL_EVENTS_TOTAL_y, ADMISSION_ELECTIVE_y, ADMISSION_EMERGENCY_y, ADMISSION_URGENT_y, ADMIT_LOCATION_CLINIC_REFERRAL/PREMATURE_y, ADMIT_LOCATION_EMERGENCY_ROOM_ADMIT_y, ADMIT_LOCATION_PHYS_REFERRAL/NORMAL_DELI_y, ADMIT_LOCATION_TRANSFER_FROM_HOSP/EXTRAM_y, ADMIT_LOCATION_TRANSFER_FROM_OTHER_HEALT_y, ADMIT_LOCATION_TRANSFER_FROM_SKILLED_NUR_y, INSURANCE_Government_y, INSURANCE_Medicaid_y, INSURANCE_Medicare_y, INSURANCE_Private_y, INSURANCE_Self_Pay_y, F_y, M_y, TEXT_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 97 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that there were no subjects after using an inner merge\n",
    "X_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating our own tokenizer, which will take strings and break them up at each space so each word is a token. We'll also use a porter stemmer to lemmatize like words. We'll also remove punctuation and any numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_tokenizer(text):\n",
    "    ps = PorterStemmer()\n",
    "    # create a dictionary mapping of characters \n",
    "    # we want to replace with what to replace with\n",
    "    punc_list = string.punctuation+'0123456789'\n",
    "    # maketrans() creates a mapping of the character's Unicode ordinal \n",
    "    t = text.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    text = text.lower().translate(t)\n",
    "    tokenized = word_tokenize(text)\n",
    "    \n",
    "    tokenized_stemmed = [ps.stem(word) for word in tokenized]\n",
    "    \n",
    "    return tokenized_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['these', 'string', 'should', 'be', 'token']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "new_tokenizer('These strings shoulD be tokenized 1/15/2021!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a count vectorizer to learn the words in the traning data (fit) and transform the data so it creates counts for each word, constrain # of words included in vectorizer which will use the top N most frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Âµs, sys: 1e+03 ns, total: 5 Âµs\n",
      "Wall time: 16.9 Âµs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyito/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=3000,\n",
       "                tokenizer=<function new_tokenizer at 0x7f8b3b58f9e0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(max_features = 3000, tokenizer = new_tokenizer)\n",
    "\n",
    "vect.fit(X_train.TEXT.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts for each word\n",
    "doc_matrix = vect.transform(X_train.TEXT.values)\n",
    "term_freq = np.sum(doc_matrix, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = np.squeeze(np.asarray(term_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>to</td>\n",
       "      <td>2796661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>2519236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>with</td>\n",
       "      <td>1694318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>of</td>\n",
       "      <td>1561204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pt</td>\n",
       "      <td>1375696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fire</td>\n",
       "      <td>2388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>drive</td>\n",
       "      <td>2385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>center</td>\n",
       "      <td>2384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pneumon</td>\n",
       "      <td>2384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fena</td>\n",
       "      <td>2382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tf\n",
       "to       2796661\n",
       "and      2519236\n",
       "with     1694318\n",
       "of       1561204\n",
       "pt       1375696\n",
       "...          ...\n",
       "fire        2388\n",
       "drive       2385\n",
       "center      2384\n",
       "pneumon     2384\n",
       "fena        2382\n",
       "\n",
       "[3000 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = np.squeeze(np.asarray(term_freq))\n",
    "tf_df = pd.DataFrame([tf], columns = vect.get_feature_names()).transpose()\n",
    "tf_df.columns = ['tf']\n",
    "tf_df.sort_values(by='tf',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.Series(tf_df.tf, index = tf_df.index).sort_values(ascending = False)\n",
    "ax = d[:50].plot(kind='bar', figsize=(10,8), width=.8, fontsize=14, rot=90, color = 'b')\n",
    "ax.title.set_size(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=['to','and','with','of','pt','for','on','s','in','ml','mg','o',\n",
    "            'at','the','l','as','a','pm','is','dl','he','she','q','h','by','his',\n",
    "            'her','k','be','but','was','name','patient','am','there','that','are',\n",
    "            'an','also', 'c','t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyito/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/lilyito/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['hi', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=3000,\n",
       "                stop_words=['to', 'and', 'with', 'of', 'pt', 'for', 'on', 's',\n",
       "                            'in', 'ml', 'mg', 'o', 'at', 'the', 'l', 'as', 'a',\n",
       "                            'pm', 'is', 'dl', 'he', 'she', 'q', 'h', 'by',\n",
       "                            'his', 'her', 'k', 'be', 'but', ...],\n",
       "                tokenizer=<function new_tokenizer at 0x7f8b3b58f9e0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add in the stop words and refit\n",
    "vect = CountVectorizer(max_features = 3000, \n",
    "                       tokenizer = new_tokenizer, \n",
    "                       stop_words = stop_words)\n",
    "\n",
    "vect.fit(X_train.TEXT.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = vect.transform(X_train.TEXT.values)\n",
    "X_test_text = vect.transform(X_test.TEXT.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print(len(vect.get_feature_names()))\n",
    "\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<22068x3000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11358911 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop identifier columns and then combine ICU data with BOW data\n",
    "X_train = X_train.drop(['TEXT','SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID','INTIME', 'OUTTIME'], axis=1)\n",
    "X_test = X_test.drop(['TEXT','SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID','INTIME', 'OUTTIME'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train_sparse = hstack([X_train, X_train_text])\n",
    "X_test_sparse = hstack([X_test, X_test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train_list + vect.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "\n",
    "import scipy\n",
    "\n",
    "scipy.sparse.save_npz('data/X_train_sparse.npz', X_train_sparse)\n",
    "scipy.sparse.save_npz('data/X_test_sparse.npz', X_test_sparse)\n",
    "\n",
    "import pickle\n",
    "with open(\"data/feature_names.txt\", \"wb\") as fp:\n",
    "    pickle.dump(feature_names, fp)\n",
    "\n",
    "with open(\"data/y_train.txt\", \"wb\") as fp:\n",
    "    pickle.dump(y_train, fp)\n",
    "    \n",
    "with open(\"data/y_test.txt\", \"wb\") as fp:\n",
    "    pickle.dump(y_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
